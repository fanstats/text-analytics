---
title: "Text Preparation with R"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This lab illusrtates the basic steps in text preparation of using R for movie reviews data.

```{r packages}
#install.packages(c('tidytext', 'dplyr', 'hunspell', 'textstem', 'qdap'))
```

## IMDB reviews
1,000 Internet Movie Database (IMDB) reviews downloaded from [machine-learning-databases](https://
archive.ics.uci.edu/ml/machine-learning-databases/00331/). The description of the
dataset is available at [link to description](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+S
entences#).

```{r dataset}
imdb <- read.table("../data/imdb_labelled.txt", col.names = c("comment","positive_flag"), quote = "", comment.char = "", sep = "\t", stringsAsFactors=FALSE)
imdb$review_id <- factor(1:nrow(imdb)) # unique identifier
head(imdb,5)

imdb$comment[c(1,283)]
```

## Tokenization
Create bags of words from the raw documents

```{r tokenization}
library(tidytext) # text mining
?tidytext
library(dplyr) # data mannipulation 
?dplyr
# tokenization
?tidytext::unnest_tokens
tidy_imdb <- imdb %>% ## pipe operator in dplyr, can be used to chain code together.
             unnest_tokens(output = word, input = comment) 
head(tidy_imdb,10)

tidy_imdb$word[which(tidy_imdb$review_id==283)]
```

## Check mis-spelling

```{r check-spelling}
library(hunspell) # Spell Checking
?hunspell
hunspell_check("moovie")
hunspell_suggest("moovie")

words <- tidy_imdb$word
checks <- hunspell_check(words)
bads <- which(!checks)
for(i in bads){ # time-consuming
  words[i] = unlist(hunspell_suggest(words[i]))[1]
}
tidy_imdb$word = words

tidy_imdb$word[1:10]
tidy_imdb$word[which(tidy_imdb$review_id==283)]
```

## Stop word removal

```{r stop-words}
stop_words # from tidytext; tibble: a reimagining of the data.frame, https://tibble.tidyverse.org/
?dplyr::anti_join
tidy_imdb <- tidy_imdb %>%
  anti_join(stop_words, by="word")
head(tidy_imdb,10)

# create new bag of stop words
?dplyr::tibble
my_stopw = tibble(word = c("movie","imdb"),lexicon = "imdbreview")
tidy_imdb <- tidy_imdb %>%
  anti_join(my_stopw, by="word")

tidy_imdb$word[1:10]
tidy_imdb$word[which(tidy_imdb$review_id==283)]
```

# Stemming and lemmatization
Reduce terms to word roots
```{r stemming}
?hunspell
hunspell_stem("movies")
library(textstem)
?textstem::lemmatize_words

words <- tidy_imdb$word
stems <- hunspell_stem(words)
for(i in 1:length(words)){
  l <- length(unlist(stems[[i]]))
  if(l>0) words[i] = stems[[i]][l]
  else words[i] = words[i]
}
words = lemmatize_words(words)
tidy_imdb$word = words

tidy_imdb$word[1:10]
tidy_imdb$word[which(tidy_imdb$review_id==283)]
```

## Replace synonyms

```{r synonyms, eval=FALSE }
library(qdap)
?qdap::syn
eg <- c("movie","watch","film")
syn(eg, report.null=F)

source("syn.R")
syn.replace(terms = eg)

uq.w = unique(tidy_imdb$word)
syn.w = syn.replace(terms = uq.w)
syn.w = syn.w$terms
for(i in 1:length(uq.w)){ # time-consuming
   ind = which(tidy_imdb$word %in% uq.w[i])
   tidy_imdb$word[ind] = syn.w[i]
}

tidy_imdb$word[1:10]
```

## Untidy the preprocessed data

```{r Subrata}
untidied_preprocessed_imdb <- as_tibble(tidy_imdb) %>% 
  group_by(positive_flag, review_id) %>% 
  summarize(text = stringr::str_c(sort(unique(word)), collapse = " ")) %>%
  ungroup() 
# https://stackoverflow.com/questions/46734501/opposite-of-unnest-tokens

#as_tibble(tidy_imdb)$word[which(tidy_imdb$review_id==283)]
untidied_preprocessed_imdb$text[which(untidied_preprocessed_imdb$review_id==283)]


untidied_preprocessed_imdb <- untidied_preprocessed_imdb %>% 
  arrange(review_id, text)
View(untidied_preprocessed_imdb)

untidied_preprocessed_imdb <- untidied_preprocessed_imdb[,2:3]
```

## Raw DTM matrix
```{r}
library(tm)
imdb_new <- array(dim=NROW(imdb))
imdb_new[untidied_preprocessed_imdb$review_id] <-  as.character(as.matrix(
  untidied_preprocessed_imdb$text ))
docs <- Corpus(VectorSource(imdb_new))
# inspect(docs)
(dtm <- TermDocumentMatrix(docs))

as.matrix(dtm)[1:10,1:10]


imdb_new[c(1,283)]
```

## tf-idf Term Document matrix
```{r}
(dtm <- TermDocumentMatrix(docs,control = list(weighting = 
                                                  function(x) weightTfIdf(x, normalize = TRUE))))

## Checked most of the empty documents are "10". One of them was "e pi"

as.matrix(dtm)[1:10,1:10]

```

